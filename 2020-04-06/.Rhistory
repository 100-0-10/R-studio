html_text()
for(addr in k) {
temp <- read_html(addr) %>% html_nodes("#article_body") %>%
html_text()
cat(temp, file = "temp.txt", append = TRUE)
}
for(url in k) {
print(url)
}
for(addr in k) {
temp <- read_html(addr) %>% html_nodes("#article_body") %>%
html_text()
cat(temp, file = "temp.txt", append = TRUE)
}
temp
temp <- read_html(addr) %>% html_nodes(".article_txt") %>%
html_text()
for(addr in k) {
temp <- read_html(addr) %>% html_nodes(".article_txt") %>%
html_text()
cat(temp, file = "temp.txt", append = TRUE)
}
# 4. 기사 검색을 url을 생성
donga <- 'http://www.donga.com/news/search?check_news=6&more=1&sorting=1&range=1&search_date=&query=%EC%BD%94%EB%A1%9C%EB%82%9819'
# 5. url에 해당하는 데이터 가져오기
addr <- read_html(donga, encoding = "utf-8")
addr
# 6. dt 태그 안에 있는 a 태그들의 href 속성의 값을 가져오기
addr <- k %>% html_nodes(".t") %>% html_nodes(".tit") %>% html_nodes("a") %>% html_attr("href")
addr
# 4. 기사 검색을 url을 생성
donga <- 'http://www.donga.com/news/search?check_news=6&more=1&sorting=1&range=1&search_date=&query=%EC%BD%94%EB%A1%9C%EB%82%9819'
# 5. url에 해당하는 데이터 가져오기
addr <- read_html(donga, encoding = "utf-8")
addr
# 6. dt 태그 안에 있는 a 태그들의 href 속성의 값을 가져오기
addr <- link %>% html_nodes(".t") %>% html_nodes(".tit") %>% html_nodes("a") %>% html_attr("href")
# 4. 기사 검색을 url을 생성
donga <- 'http://www.donga.com/news/search?check_news=6&more=1&sorting=1&range=1&search_date=&query=%EC%BD%94%EB%A1%9C%EB%82%9819'
# 5. url에 해당하는 데이터 가져오기
link <- read_html(donga, encoding = "utf-8")
link
# 6. dt 태그 안에 있는 a 태그들의 href 속성의 값을 가져오기
addr <- link %>% html_nodes(".t") %>% html_nodes(".tit") %>% html_nodes("a") %>% html_attr("href")
addr
for(url in addr) {
print(url)
}
for(url in addr) {
content <- read_html(url) %>% html_nodes(".article_txt") %>%
html_text()
cat(content, file = "temp.txt", append = TRUE)
}
for(url in addr) {
content <- read_html(url) %>% html_nodes(".article_txt") %>%
html_text()
cat(content, file = "temp.txt", append = TRUE)
}
# 4. 기사 검색을 url을 생성
donga <- 'http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1'
# 5. url에 해당하는 데이터 가져오기
link <- read_html(donga, encoding = "utf-8")
link
# 6. dt 태그 안에 있는 a 태그들의 href 속성의 값을 가져오기
addr <- link %>% html_nodes(".t") %>% html_nodes(".tit") %>% html_nodes("a") %>% html_attr("href")
addr
for(url in addr) {
print(url)
}
for(url in addr) {
content <- read_html(url) %>% html_nodes(".article_txt") %>%
html_text()
cat(content, file = "temp.txt", append = TRUE)
}
temp
content
# 4. 기사 검색을 url을 생성
donga <- 'http://www.donga.com/news/search?check_news=6&more=1&sorting=1&range=1&search_date=&query=%EC%BD%94%EB%A1%9C%EB%82%9819'
# http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1
# 5. url에 해당하는 데이터 가져오기
link <- read_html(donga, encoding = "utf-8")
link
# 6. dt 태그 안에 있는 a 태그들의 href 속성의 값을 가져오기
addr <- link %>% html_nodes(".t") %>% html_nodes(".tit") %>% html_nodes("a") %>% html_attr("href")
addr
for(url in addr) {
print(url)
}
content
for(url in addr) {
content <- read_html(url) %>% html_nodes(".article_txt") %>%
html_text()
cat(content, file = "temp.txt", append = TRUE)
}
for(url in addr) {
content <- read_html(url) %>% html_nodes(".article_txt") %>%
html_text()
cat(content, file = "temp.txt", append = TRUE)
}
# 8. 파일의 모든 내용 가져오기
txt <- readLines("content.txt")
head(txt)
# 8. 파일의 모든 내용 가져오기
txt <- readLines("temp.txt")
head(txt)
txt <- str_replace_all(txt, "\\W", " ")
# 9. 명사만 추출
useNIADic()
nouns <- extractNoun(txt)
nouns
# 10. 빈도수 만들기 - 각 단어가 몇 번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 11. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringAsFactors = F)
df_word
class(df_word)
# 12. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 13. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 14. 시드 설정
set.seed(1234)
# 15. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.words = 200,
random.order = F,
rot.per = .1, scale = c(4, 0.3), colors = pal)
# scan()함수 이용
x <- scan()
x
x <- scan(what= " ")
age = data.frame()
age = edit(age)
age
useSejongDic()
library(KoNLP)
library(wordcloud)
library(stringr)
useSejongDic()
# 정규 표현식 예
text <- c("phone: 010-1234-5678",
"home: 02-123-1123",
"이름:홍길동")
grep("[[:digit:]]", text, value = T)
grep("[[:digit:]]", text, value = T)
# 12. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 12. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 4. 기사 검색을 url을 생성
donga <- 'http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1'
# http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1
# 5. url에 해당하는 데이터 가져오기
link <- read_html(donga, encoding = "utf-8")
link
library(memoise)
library(dplyr)
# 3. 웹에서 문자열을 가져와서 html 파싱을 하기 위한 패키지 로드
library(rvest)
# 4. 기사 검색을 url을 생성
donga <- 'http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1'
# http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1
# 5. url에 해당하는 데이터 가져오기
link <- read_html(donga, encoding = "utf-8")
link
# 6. dt 태그 안에 있는 a 태그들의 href 속성의 값을 가져오기
addr <- link %>% html_nodes(".t") %>% html_nodes(".tit") %>% html_nodes("a") %>% html_attr("href")
addr
for(url in addr) {
print(url)
}
for(url in addr) {
content <- read_html(url) %>% html_nodes(".article_txt") %>%
html_text()
cat(content, file = "temp.txt", append = TRUE)
}
content
# 8. 파일의 모든 내용 가져오기
txt <- readLines("temp.txt")
# 8. 파일의 모든 내용 가져오기
txt <- readLines("temp.txt")
head(txt)
content
# 8. 파일의 모든 내용 가져오기
txt <- readLines("temp.txt")
gsub("[[:digit:]]", "x", text)
# 8. 파일의 모든 내용 가져오기
txt <- readLines("temp.txt")
# 8. 파일의 모든 내용 가져오기
txt <- readLines("temp.txt")
head(txt)
txt <- str_replace_all(txt, "\\W", " ")
# 9. 명사만 추출
useNIADic()
nouns <- extractNoun(txt)
nouns
# 10. 빈도수 만들기 - 각 단어가 몇 번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 11. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringAsFactors = F)
df_word
# 12. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 한글 문자열분석에서 매우 중요한 정규표현식.
grep("[가-핳]", text, value=T)
grep("[[:lower:]]", text, value=T)
View(df_word)
# 4. 기사 검색을 url을 생성
donga <- 'http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1'
# http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1
# 5. url에 해당하는 데이터 가져오기
link <- read_html(donga, encoding = "utf-8")
link
# 6. dt 태그 안에 있는 a 태그들의 href 속성의 값을 가져오기
addr <- link %>% html_nodes(".t") %>% html_nodes(".tit") %>% html_nodes("a") %>% html_attr("href")
addr
for(url in addr) {
print(url)
}
for(url in addr) {
content <- read_html(url) %>% html_nodes(".article_txt") %>%
html_text()
cat(content, file = "temp.txt", append = TRUE)
}
content
# 8. 파일의 모든 내용 가져오기
txt <- readLines("temp.txt")
gsub("^[1-9][0-9]*$", " ", c("08", "1", "19 189", "78"))
# 4. 기사 검색을 url을 생성
donga <- 'http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1'
# http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&v1=&v2&range=1
# 5. url에 해당하는 데이터 가져오기
link <- read_html(donga, encoding = "utf-8")
link
# 6. dt 태그 안에 있는 a 태그들의 href 속성의 값을 가져오기
addr <- link %>% html_nodes(".t") %>% html_nodes(".tit") %>% html_nodes("a") %>% html_attr("href")
addr
for(url in addr) {
print(url)
}
for(url in addr) {
content <- read_html(url) %>% html_nodes(".article_txt") %>%
html_text()
cat(content, file = "article.txt", append = TRUE)
}
content
# 8. 파일의 모든 내용 가져오기
txt <- readLines("article.txt")
head(txt)
txt <- str_replace_all(txt, "\\W", " ")
# 9. 명사만 추출
useNIADic()
nouns <- extractNoun(txt)
nouns
# 10. 빈도수 만들기 - 각 단어가 몇 번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 11. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word)
# 12. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 13. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 14. 시드 설정
set.seed(1234)
# 15. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.words = 200,
random.order = F,
rot.per = .1, scale = c(4, 0.3), colors = pal)
gsub("^[0-9]+(\\.[0-9]{1,2})?$", "zz",
c("123","123.17","123.456","123.","12.79"))
mergeUserDic(data.frame(readLines("/서울명소merge.txt"), "ncn"))
txt <- readLines("/seoul_go.txt")
mergeUserDic(data.frame(readLines("서울명소merge.txt"), "ncn"))
txt <- readLines("seoul_go.txt")
place <- sapply(txt, extractNoun, USE.NAMES = F)
head(place, 10)
head(unlist(place), 30)
place
head(place, 10)
head(txt, 10)
head(place, 10)
head(txt, 10)
head(place, 10)
head(unlist(place), 30)
# 영어제거
c <- unlist(place)
res <- str_replace_all(c, "[^[:alpha:]]", "")
res
head(res, 10)
head(res, 30)
txt <- readLines("서울명소gsub.txt")
txt
cnt_txt <- length(txt)
cnt_txt
res <- gsub((txt[i]), "", res)
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
res <- str_replace_all(c, "[^[:alpha:]]", "")
head(res, 30)
txt <- readLines("서울명소gsub.txt")
txt
cnt_txt <- length(txt)
cnt_txt
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
txt <- readLines("서울명소gsub.txt")
txt
cnt_txt <- length(txt)
cnt_txt
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
# 영어제거
c <- unlist(place)
res <- str_replace_all(c, "[^[:alpha:]]", "")
head(res, 30)
txt <- readLines("서울명소gsub.txt")
txt
cnt_txt <- length(txt)
cnt_txt
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
mergeUserDic(data.frame(readLines("서울명소merge.txt"), "ncn"))
res
res2 <- Filter(function(x) {nchar(x) >= 2}, res)
nrow(res2)
res
# 영어제거
c <- unlist(place)
res <- str_replace_all(c, "[^[:alpha:]]", "")
head(res, 30)
txt <- readLines("서울명소gsub.txt")
txt
cnt_txt <- length(txt)
cnt_txt
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
res
for(i in 1:cnt_txt) {
res <- gsub((txt[i]), "", res)
}
res
res2 <- Filter(function(x) {nchar(x) >= 2}, res)
nrow(res2)
write(res2, "./result_files/seoul_go2.txt")
write(res2, "seoul_go2.txt")
res3 <- read.table("seoul_go2.txt")
res3
wordcount <- table(res3)
head(sort(wordcount, decreasing = T), 30)
palete <- brewer.pal(8, "Set2")
wordcloud(names(wordcount),
freq=wordcount,
scale = c(3, 1),
rot.per = 0.25,
min.freq = 5,
random.order = F,
random.color = T,
colors = palete)
legend(0.3,
1,
"블로거 추천 서울 명소 분석    ",
cex = 0.6,
fill = NA,
border = NA,
bg = "white",
text.col = "red",
text.font = 2,
box.col = "red")
legend(0.3,
1,
"블로거 추천 서울 명소 분석    ",
cex = 0.6,
fill = NA,
border = NA,
bg = "white",
text.col = "red",
text.font = 2,
box.col = "red")
legend(0.3,
1,
"블로거 추천 서울 명소 분석    ",
cex = 0.6,
fill = NA,
border = NA,
bg = "white",
text.col = "red",
text.font = 2,
box.col = "red")
legend(0.3,
1,
"블로거 추천 서울 명소 분석    ",
cex = 0.6,
fill = NA,
border = NA,
bg = "white",
text.col = "red",
text.font = 2,
box.col = "red")
wordcloud(names(wordcount),
freq=wordcount,
scale = c(3, 1),
rot.per = 0.25,
min.freq = 5,
random.order = F,
random.color = T,
colors = palete)
legend(0.3,
1,
"블로거 추천 서울 명소 분석    ",
cex = 0.6,
fill = NA,
border = NA,
bg = "white",
text.col = "red",
text.font = 2,
box.col = "red")
wordcloud(names(wordcount),
freq=wordcount,
scale = c(3, 1),
rot.per = 0.25,
min.freq = 5,
random.order = F,
random.color = T,
colors = palete)
legend(0.3,
1,
"블로거 추천 서울 명소 분석    ",
cex = 0.6,
fill = NA,
border = NA,
bg = "white",
text.col = "red",
text.font = 2,
box.col = "red")
legend(0.3,
1,
"블로거 추천 서울 명소 분석석",
cex = 0.6,
fill = NA,
border = NA,
bg = "white",
text.col = "red",
text.font = 2,
box.col = "red")
legend(0.3,
1,
"블로거 추천 서울 명소 분석",
cex = 0.6,
fill = NA,
border = NA,
bg = "white",
text.col = "red",
text.font = 2,
box.col = "red")
wordcloud(names(wordcount),
freq=wordcount,
scale = c(3, 1),
rot.per = 0.25,
min.freq = 5,
random.order = F,
random.color = T,
colors = palete)
mergeUserDic(data.frame("제주도여행지.txt"), "ncn")
txt <- readLines("jeju.txt")
txt
head(txt, 30)
head(txt, 10)
place <- sapply(txt, extractNoun, USE.NAMES = F)
View(place)
View(place)
